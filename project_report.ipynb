{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4 Report\n",
    "\n",
    "This is my report by Project 4 of the Machine Learning Engineer Nanodegree - **Teach a Smartcab How to Drive**. In this project I used reinforcement learning techniques to teach an engine how to play a simple game of reaching a destination on a grid-like world given some restrictions. The project description can be found [here](https://classroom.udacity.com/nanodegrees/nd009/parts/0091345409/modules/540405889375461/lessons/5404058893239847/concepts/54440204820923), and my code for the project is on [this github repo](https://github.com/lmurtinho/machine-learning/tree/my_projects/projects/smartcab).\n",
    "\n",
    "## Task 0: Implementing a Perfect Agent\n",
    "\n",
    "Although this is not a requirement, it is relatively simple, given the inputs, to define a set of rules that result in an agent that will always take the right action. The next move is given by the `self.next_waypoint` variable inside the `update` method of our agent; the only thing to do is take care that the agent will only perform the next move when it can, following the right-of-way rules in the project description, reproduced below:\n",
    "\n",
    ">US right-of-way rules apply: On a green light, you can turn left only if there is no oncoming traffic at the intersection coming straight. On a red light, you can turn right if there is no oncoming traffic turning left or traffic from the left going straight.\n",
    "\n",
    "So, a simple set of rules should suffice to make our agent perfect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is a method of an Agent object\n",
    "def update(self, t):\n",
    "    # Gather inputs\n",
    "    self.next_waypoint = self.planner.next_waypoint()  # from route planner, also displayed by simulator\n",
    "    inputs = self.env.sense(self)\n",
    "    deadline = self.env.get_deadline(self)\n",
    "    \n",
    "    # The next best move is given by the planner\n",
    "    action = self.next_waypoint\n",
    "    \n",
    "    # On a red light, the agent can only turn right, and even so only if:\n",
    "    # - no oncoming traffic is going left\n",
    "    # - no traffic from the left is going forward\n",
    "    if inputs['light'] == 'red':\n",
    "        if (action != 'right') or (inputs['oncoming'] == 'left') or (inputs['left'] == 'forward'):\n",
    "            action = None\n",
    "    \n",
    "    # On a green light, the agent cannot turn left if there is\n",
    "    # oncoming traffic going forward\n",
    "    elif inputs['oncoming'] == 'forward' and action == 'left':\n",
    "        action = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"perfect agent\" version of the code is stored in [this Github commit](https://github.com/lmurtinho/machine-learning/tree/c743b492c1f1b033ff0724fece542e6a01901540).\n",
    "\n",
    "However, the goal of the project is to build an agent that can *learn these rules by itself*. Let's follow the project's rubric and see how it goes.\n",
    "\n",
    "## Task 1: Implement a Basic Driving Agent\n",
    "\n",
    "According to the project's instructions, the basic driving agent should \"produce some random move/action.\" That's easy enough: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def update(self, t):\n",
    "    # Gather inputs\n",
    "    self.next_waypoint = self.planner.next_waypoint()  # from route planner, also displayed by simulator\n",
    "    inputs = self.env.sense(self)\n",
    "    deadline = self.env.get_deadline(self)\n",
    "    \n",
    "    # Do something random\n",
    "    action = random.choice(['right', 'left', 'forward', None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this strategy, the agent may reach its destination on time, but only if it gets lucky. When the deadline is not enforced, the agent will eventually get there - but it can take a long time, since it's not at all actually aiming for it. The agent also gets lots of penalties for incurring in illegal moves, such as trying to go forward when the light is red. \n",
    "\n",
    "The \"random agent\" version of the code is stored in [this Github commit](https://github.com/lmurtinho/machine-learning/tree/bbdc04d70cc31e2b8071bcf8356e9b752aed29ef).\n",
    "\n",
    "From this behavior (and the behavior of the perfect agent implemented before) we can begin to think about what information needs to be in the state for the agent to learn the appropriate behavior: it needs to know where the destination is, as well as information about its surroundings (if the light is green or red and whether right-of-way rules imply it should stay put). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Identify and Update State\n",
    "\n",
    "The following information is available for the agent at each update:\n",
    "\n",
    "- *Light*: whether the light is red or green. As mentioned above, a green light means the agent can perform the next action, with the possible exception of a left turn, while a red light means the agent should stay put, with the possible exception of a right turn. So, it is important to add the light to the state.\n",
    "- *Oncoming*: whether there is oncoming traffic, and which direction it is going. As mentioned above, oncoming traffic may mean the agent cannot turn left or right, so this information needs to be in the state as well.\n",
    "- *Right*: whether there is traffic from the right of the agent, and which direction it is going. Right-of-way rules don't mention traffic to the right at any point, so this is unnecessary information that doesn't need to be in the state for the agent to learn the optimal policy.\n",
    "- *Left*: whether there is traffic from the left of the agent, and which direction it is going. Traffic from the left going straight means the agent cannot turn right on a red light, so this needs to be in the state.\n",
    "- *Next waypoint*: the direction the agent should go to reach the destination. Without this information, the agent does not know where to go next and might as well wonder around randomly, so this needs to go in the state.\n",
    "- *Deadline*: how much time the agent has left to reach its destination. At first, I would say this is not meaningful information for the agent, since it doesn't change right-of-way rules nor the best route. I thought about adding it to the state anyway, but this would mean a large increase in the number of possible states. Using only `light` (red or green), `oncoming` (None, left, right, or forward), `left` (None, left, right, or forward) and `next_waypoint` (left, right, or forward), we have $2\\times4\\times4\\times3=96$ possible states. Adding `deadline` would mean multiplying this number by 50, if not more. So I'll keep `deadline` off my state for now.\n",
    "\n",
    "There is the possibility of combining inputs to create a state. Maybe I could define the state in such a way that the allowed actions would be immediately available for the agent; for instance, I could come up with a `turn_right` variable that would check if the agent can turn right, and add that variable to the state. At least for now, though, I'd rather see how the agent deals with the \"raw\" variables; I can tweak the state later to try and get the agent to find the optimal policy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def update(self, t):\n",
    "    # Gather inputs\n",
    "    self.next_waypoint = self.planner.next_waypoint()  # from route planner, also displayed by simulator\n",
    "    inputs = self.env.sense(self)\n",
    "    deadline = self.env.get_deadline(self)\n",
    "    \n",
    "    # update state\n",
    "    self.state = (inputs['light'], inputs['oncoming'], inputs['left'],\n",
    "                  self.next_waypoint)\n",
    "    \n",
    "    # Do something random\n",
    "    action = random.choice(['right', 'left', 'forward', None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The version of the code with a state is stored in [this Github commit](https://github.com/lmurtinho/machine-learning/tree/f5a6241f8d29a6d53e98ecc79715cd69947ad348)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Task 3: Implement Q-Learning\n",
    "In this step our agent begins to learn from its actions. The project specifically says to \"pick the best action available from the current state based on Q-values,\" so I'll leave the exploration-exploitation dilemma for the next section.\n",
    "\n",
    "### Deciding on the Appropriate Q-learning Function \n",
    "\n",
    "The general form of the $Q$-function is: \n",
    "\n",
    "$Q(s, a) = R(s) + \\gamma\\underset{s'}\\sum T(s,a,s')\\underset{a'}{\\operatorname{max}}Q(s', a')$\n",
    "\n",
    "That is, the $Q$-value for a given `(state, action)` pair is the the reward for that state, $R(s)$, plus the discounted expected value of $Q$ for the next state the agent lands in, considering the transition function $T(s,a,s') = \\Pr(s' \\mid s,a)$ (the probability of landing on state $s'$ coming from state $s$ and performing action $a$) and that, whatever $s'$ is, the agent will maximize $Q$ from there on.\n",
    "\n",
    "The $Q$-learning update function is given by:\n",
    "\n",
    "$\\hat{Q}_t(s, a) = (1 - \\alpha_t)\\hat{Q}_{t-1}(s, a) + \\alpha_t(r + \\gamma\\underset{a'}{\\operatorname{max}}\\hat{Q}_{t-1}(s', a'))$\n",
    "\n",
    "That is, our estimate of the $Q$-value for the `(state, action)` pair is updated with the learning rate ($\\alpha_t$, which varies over time) by the observed reward ($r$) and our previous estimate of the Q-value for the observed next state ($s'$), discounted by the discount factor ($\\gamma$) and considering the agent will pick the action $a'$ that maximizes $Q$ from the next state on.\n",
    "\n",
    "However, in this case there's no need to worry about the future state, since the agent gets an immediate reward for doing the right thing. According to the project description:\n",
    "\n",
    ">The smartcab gets a reward for each successfully completed trip. A trip is considered “successfully completed” if the passenger is dropped off at the desired destination (some intersection) within a pre-specified time bound (computed with a route plan).\n",
    "\n",
    ">It also gets a smaller reward for each correct move executed at an intersection. It gets a small penalty for an incorrect move, and a larger penalty for violating traffic rules and/or causing an accident.\n",
    "\n",
    "So, even though the larger reward is only reaped once the agent reaches its destination, there are smaller rewards for following the correct path, and penalties for not doing so. This should be enough for the agent to learn the best policy.\n",
    "\n",
    "Granted, ignoring the agent's future decisions means I'm not using some information that could be of help. But the upside is a simplification of the problem: it's as if the agent is playing a 1-round game over and over, with immediate rewards for immediate actions. I expect this simplification more than compensates ignoring long-term rewards in this particular setting.\n",
    "\n",
    "### Q-learning implementation\n",
    "\n",
    "This means I won't actually bother with keeping track of the state the agent ends up in after performing an action (or, to be more technical, I'm setting the discount factor $\\gamma$ to zero). My update function will then simply be $\\hat{Q}_t(s, a) = (1 - \\alpha_t)\\hat{Q}_{t-1}(s, a) + \\alpha_tr$.\n",
    "\n",
    "Here's what I'll do:\n",
    "- Initialize `agent.qvals` as an empty dictionary and `agent.time` as 0 when the agent is initialized. \n",
    "- Define a `best_action()` method that takes a state and returns the best action (or one of the best actions) given the current Q-values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# agent method\n",
    "def best_action(self, state):\n",
    "    \"\"\"\n",
    "    Returns the best action (the one with the maximum Q-value)\n",
    "    or one of the best actions, given a state.\n",
    "    \"\"\"\n",
    "    # get all possible q-values for the state \n",
    "    all_qvals = {action: self.qvals.get((state, action), 0)\n",
    "                 for action in self.possible_actions}        \n",
    "\n",
    "    # pick the actions that yield the largest q-value for the state\n",
    "    best_actions = [action for action in self.possible_actions \n",
    "                    if all_qvals[action] == max(all_qvals.values())]\n",
    "\n",
    "    # return one of the best actions at random\n",
    "    return random.choice(best_actions)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the `update()` agent method: \n",
    "    - increment the time by 1\n",
    "    - set the learning rate as 1/time\n",
    "    - pick the action using `best_action()`\n",
    "    - update the value for the `(state, action)` pair in the `qvals` with the reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# agente method\n",
    "def update(self, t):\n",
    "    # Gather inputs\n",
    "    self.next_waypoint = self.planner.next_waypoint()  # from route planner, also displayed by simulator\n",
    "    inputs = self.env.sense(self)\n",
    "    deadline = self.env.get_deadline(self)\n",
    "\n",
    "    # update time and learning rate\n",
    "    self.time += 1\n",
    "    learn_rate = 1.0 / self.time\n",
    "\n",
    "    # Update state\n",
    "    self.state = (inputs['light'], inputs['oncoming'], inputs['left'],\n",
    "                  self.next_waypoint)\n",
    "\n",
    "    # Pick the best known action\n",
    "    action = self.best_action(self.state)\n",
    "\n",
    "    # Execute action and get reward\n",
    "    reward = self.env.act(self, action)\n",
    "\n",
    "    # Update the q-value of the (state, action) pair\n",
    "    self.qvals[(self.state, action)] = \\\n",
    "        (1 - learn_rate) * self.qvals.get((self.state, action), 0) + \\\n",
    "        learn_rate * reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version of the code is stored in [this Github commit](https://github.com/lmurtinho/machine-learning/tree/0a81431628b75a021c2698c2f2897c0032d39db7)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Enhance the Driving Agent\n",
    "\n",
    "The last task, according to the project description, is:\n",
    "\n",
    "> Apply the reinforcement learning techniques you have learnt, and tweak the parameters (e.g. learning rate, discount factor, action selection method, etc.), to improve the performance of your agent. Your goal is to get it to a point so that within 100 trials, the agent is able to learn a feasible policy - i.e. reach the destination within the allotted time, with net reward remaining positive.\n",
    "\n",
    "This goal is reached with the implementation presented above. Although there are instances in which, with 90+ trials, the agent will pick a wrong action from time to time, in my tests it rarely failed to reach the destination after 60+ trials.\n",
    "\n",
    "But that doesn't mean the agent can't be modified, and it certainly means it can be improved. According to [one of the videos](https://www.udacity.com/course/viewer#!/c-ud728-nd/l-5446820041/m-634899064) in the Reinforcement Learning course, there are three characteristics that can change a Q-learning algorithm:\n",
    "\n",
    "- How Q-values are initialized\n",
    "- How the learning rate decays\n",
    "- How the action is picked\n",
    "\n",
    "I'll change each one of these three characteristics at a time, and then I'll tackle everything at once to see it I can get a better learning agent.\n",
    "\n",
    "### Baseline Comparison\n",
    "\n",
    "Before modifying my agent, I need to define what a \"better\" agent would look like. Some ideas come to mind:\n",
    "\n",
    "- A better agent would have a larger sum of rewards over time\n",
    "- A better agent would reach its destination more often\n",
    "- A better agent would learn faster\n",
    "- A better agent would make fewer mistakes\n",
    "- A better agent would explore as much of the world as possible\n",
    "\n",
    "Some of these may be redundant in this setting: more rewards, for instance, probably means fewer mistakes, and vice-versa. Still, I think this is a good starting point. So I'll keep track of the following variables:\n",
    "\n",
    "- `reward_sum`: the sum of rewards for an agent over a simulation\n",
    "- `disc_reward_sum`: the sum of rewards *discounted over time*, so that an agen that heaps larger rewards faster (i.e. learns faster) will have a larger `disc_reward`\n",
    "- `n_dest_reached`: the number of times the agent has reached its destination\n",
    "- `last_dest_fail`: the most recent trial in which the agent did not reach its destination\n",
    "- `sum_time_left`: the sum over all trials of the steps the agent still had available when it reached its destination (0 if it never reached it)\n",
    "- `n_penalties`: the number of penalties incurred by the agent\n",
    "- `last_penalty`: the most recent trial in which the agent received a penalty\n",
    "- `len_qvals`: how many Q-values are mapped in the agent's Q-function - that is, how many `(state, action)` pairs it visited during the simulation\n",
    "\n",
    "Here are the basic statistics for my baseline agent, after running 100 simulations with 100 trials each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reward_sum</th>\n",
       "      <th>disc_reward_sum</th>\n",
       "      <th>n_dest_reached</th>\n",
       "      <th>last_dest_fail</th>\n",
       "      <th>sum_time_left</th>\n",
       "      <th>n_penalties</th>\n",
       "      <th>last_penalty</th>\n",
       "      <th>len_qvals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2218.360000</td>\n",
       "      <td>76.857365</td>\n",
       "      <td>99.240000</td>\n",
       "      <td>16.370000</td>\n",
       "      <td>1747.400000</td>\n",
       "      <td>31.250000</td>\n",
       "      <td>92.920000</td>\n",
       "      <td>49.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>40.326764</td>\n",
       "      <td>21.534641</td>\n",
       "      <td>1.006243</td>\n",
       "      <td>29.570889</td>\n",
       "      <td>67.273751</td>\n",
       "      <td>17.244162</td>\n",
       "      <td>5.864945</td>\n",
       "      <td>6.780841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2124.000000</td>\n",
       "      <td>40.248500</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1516.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>34.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2185.875000</td>\n",
       "      <td>61.049489</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1710.000000</td>\n",
       "      <td>25.750000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>44.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2217.750000</td>\n",
       "      <td>73.482848</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1754.500000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>49.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2244.875000</td>\n",
       "      <td>87.325426</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>11.250000</td>\n",
       "      <td>1789.500000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>54.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2313.500000</td>\n",
       "      <td>150.830346</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1888.000000</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>68.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        reward_sum  disc_reward_sum  n_dest_reached  last_dest_fail  \\\n",
       "count   100.000000       100.000000      100.000000      100.000000   \n",
       "mean   2218.360000        76.857365       99.240000       16.370000   \n",
       "std      40.326764        21.534641        1.006243       29.570889   \n",
       "min    2124.000000        40.248500       92.000000        0.000000   \n",
       "25%    2185.875000        61.049489       99.000000        0.000000   \n",
       "50%    2217.750000        73.482848       99.000000        1.000000   \n",
       "75%    2244.875000        87.325426      100.000000       11.250000   \n",
       "max    2313.500000       150.830346      100.000000      100.000000   \n",
       "\n",
       "       sum_time_left  n_penalties  last_penalty   len_qvals  \n",
       "count     100.000000   100.000000    100.000000  100.000000  \n",
       "mean     1747.400000    31.250000     92.920000   49.400000  \n",
       "std        67.273751    17.244162      5.864945    6.780841  \n",
       "min      1516.000000    20.000000     72.000000   34.000000  \n",
       "25%      1710.000000    25.750000     90.000000   44.000000  \n",
       "50%      1754.500000    28.000000     94.000000   49.000000  \n",
       "75%      1789.500000    32.000000     98.000000   54.000000  \n",
       "max      1888.000000   162.000000    100.000000   68.000000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_original = pd.DataFrame.from_csv(\"original_agent_results.csv\")\n",
    "df_original.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(The csv file with the results described above is [here](https://github.com/lmurtinho/machine-learning/blob/63dbdf6cef703a2ea8e5839bcd400bc3f46f7aaf/projects/smartcab/original_agent_results.csv).)\n",
    "\n",
    "`reward_sum`, `disc_reward_sum`, `sum_time_left`, `n_penalties` and `len_qvals` will make more sense when compared with the results from other agents, but there's already something to be said about the other variables:\n",
    "\n",
    "- In all simulations, more than 50% of destinations are reached on time, and in most simulations only one destination is not reached. So the primary goal of getting to the destination is being met.\n",
    "- For most simulations, after 35 trials there is no more failures to reach a destination. But there are some simulations in which later trials result in failures.\n",
    "- The agent keeps getting penalties (for an incorrect or invalid move) even quite late in most simulations - a sign that the earlier trials could use more exploration and less exploitation.\n",
    "\n",
    "Overall, this still seems like a solid learning agent, but let's see if some tweaks can make it better.\n",
    "\n",
    "### Changing Initial Q-Values\n",
    "\n",
    "One Q-learning implementation briefly discussed in the Reinforcement Learning lessons for this project is \"optimism in the face of uncertainty\". The idea is that high initial Q-values (implying an \"optimistic\" agent in the sense that it initially believes all possible actions will yield excellent rewards) lead to an explorative agent, because it will delay exploiting familiar paths, since those will end up with lower Q-values than its initial estimate.\n",
    "\n",
    "All it takes for my original agent to become optimistic is changing the value it gets when the `(state, action)` pair is not yet a key in `qvals`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def best_action(self, state):\n",
    "    \"\"\"\n",
    "    Returns the best action (the one with the maximum Q-value)\n",
    "    or one of the best actions, given a state.\n",
    "    \"\"\"        \n",
    "    # get all possible q-values for the state\n",
    "    # (be optimistic in the face of uncertainty)\n",
    "    all_qvals = {action: self.qvals.get((state, action), 5)\n",
    "                 for action in self.possible_actions}        \n",
    "\n",
    "    # pick the actions that yield the largest q-value for the state\n",
    "    best_actions = [action for action in self.possible_actions \n",
    "                    if all_qvals[action] == max(all_qvals.values())]\n",
    "\n",
    "    # return one of the best actions at random\n",
    "    return random.choice(best_actions)        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of getting a 0 for previously unknown `(state, action)` pairs, the agent now gets a 5, which is more than even a correct move would yield as reward. So, it has more of an incentive to investigate new paths.\n",
    "\n",
    "I also changed the Q-learning update function, as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update(self, t):\n",
    "    # Gather inputs\n",
    "    self.next_waypoint = self.planner.next_waypoint()  # from route planner, also displayed by simulator\n",
    "    inputs = self.env.sense(self)\n",
    "    deadline = self.env.get_deadline(self)\n",
    "\n",
    "    # update time and learning rate\n",
    "    self.time += 1\n",
    "    learn_rate = 1.0 / self.time\n",
    "\n",
    "    # Update state\n",
    "    self.state = (inputs['light'], inputs['oncoming'], inputs['left'],\n",
    "                  self.next_waypoint)\n",
    "\n",
    "    # Pick the best known action\n",
    "    action = self.best_action(self.state)\n",
    "\n",
    "    # Execute action and get reward\n",
    "    reward = self.env.act(self, action)\n",
    "    if reward < 0:\n",
    "        self.n_penalties += 1\n",
    "    self.reward_sum += reward\n",
    "    self.disc_reward_sum += reward / (self.time/10.0)\n",
    "\n",
    "    # Update the q-value of the (state, action) pair\n",
    "    # be optimistic in the face of uncertainty\n",
    "    self.qvals[(self.state, action)] = \\\n",
    "        (1 - learn_rate) * self.qvals.get((self.state, action), 5) + \\\n",
    "        learn_rate * reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the results for running 100 simulations of 100 trials each with this optimistic agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reward_sum</th>\n",
       "      <th>disc_reward_sum</th>\n",
       "      <th>n_dest_reached</th>\n",
       "      <th>last_dest_fail</th>\n",
       "      <th>sum_time_left</th>\n",
       "      <th>n_penalties</th>\n",
       "      <th>last_penalty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1406.150000</td>\n",
       "      <td>25.556748</td>\n",
       "      <td>55.280000</td>\n",
       "      <td>98.230000</td>\n",
       "      <td>784.930000</td>\n",
       "      <td>998.800000</td>\n",
       "      <td>99.970000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>313.708878</td>\n",
       "      <td>13.358804</td>\n",
       "      <td>13.900134</td>\n",
       "      <td>2.684994</td>\n",
       "      <td>224.259106</td>\n",
       "      <td>166.474374</td>\n",
       "      <td>0.222702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>410.000000</td>\n",
       "      <td>-4.317224</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>233.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>98.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1285.250000</td>\n",
       "      <td>17.793487</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>97.750000</td>\n",
       "      <td>650.750000</td>\n",
       "      <td>872.750000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1465.750000</td>\n",
       "      <td>23.965617</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>773.000000</td>\n",
       "      <td>986.500000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1632.125000</td>\n",
       "      <td>32.683377</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>944.500000</td>\n",
       "      <td>1126.750000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1993.000000</td>\n",
       "      <td>93.368344</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1523.000000</td>\n",
       "      <td>1450.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        reward_sum  disc_reward_sum  n_dest_reached  last_dest_fail  \\\n",
       "count   100.000000       100.000000      100.000000      100.000000   \n",
       "mean   1406.150000        25.556748       55.280000       98.230000   \n",
       "std     313.708878        13.358804       13.900134        2.684994   \n",
       "min     410.000000        -4.317224       17.000000       88.000000   \n",
       "25%    1285.250000        17.793487       48.000000       97.750000   \n",
       "50%    1465.750000        23.965617       57.000000       99.000000   \n",
       "75%    1632.125000        32.683377       64.000000      100.000000   \n",
       "max    1993.000000        93.368344       93.000000      100.000000   \n",
       "\n",
       "       sum_time_left  n_penalties  last_penalty  \n",
       "count     100.000000   100.000000    100.000000  \n",
       "mean      784.930000   998.800000     99.970000  \n",
       "std       224.259106   166.474374      0.222702  \n",
       "min       233.000000   569.000000     98.000000  \n",
       "25%       650.750000   872.750000    100.000000  \n",
       "50%       773.000000   986.500000    100.000000  \n",
       "75%       944.500000  1126.750000    100.000000  \n",
       "max      1523.000000  1450.000000    100.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_optimistic = pd.DataFrame.from_csv(\"optimistic_agent_results.csv\")\n",
    "df_optimistic.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(The csv file is [here](https://github.com/lmurtinho/machine-learning/blob/5cd532b2a13dbf593a6e3605e4594bfa81d11018/projects/smartcab/optimistic_agent_results.csv).)\n",
    "\n",
    "A clearly *worse* result than the original agent! The next modification will help explain why.\n",
    "\n",
    "### Modifying the Learning Rate Decay\n",
    "\n",
    "My `learn_rate` variable is the inverse of my `time` variable - that is, when `time` is 1, `learn_rate` is 1/1; when `time` is 2, `learn_rate` is 1/2, and so on.\n",
    "\n",
    "But that means that the learning rate goes down *even for the* `(state, action)` *pairs not yet visited*, which greatly affects how states are updated. For example, suppose a learning agent with optimistic initialization of Q-values such that all `(state, action)` pairs initially map to a Q-value of 3. Let's also assume that the true reward for action x is -1 for both states A and B. If A is the initial state and x is the first action taken, and assuming a learning rate that is the inverse of time, the `(A, x)` Q-value will be updated to $(1 - 1)\\times3 + 1\\times-1 = -1$. If state B is the state at time 2 and action x is taken again, however, the `(B, x) ` Q-value will be updted to $(1-1/2)\\times3 + (1/2)\\times-1 = 1$. So the Q-value will be quite different for both `(state, action)` pairs, even though the agent visited them both only once.\n",
    "\n",
    "To solve this, let's change `learn_rate` to take into account the agent's experience *in a given* `(state, action)` *pair*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# self.visits is initialized as empty dict\n",
    "def update(self, t):\n",
    "    # Gather inputs\n",
    "    self.next_waypoint = self.planner.next_waypoint()  # from route planner, also displayed by simulator\n",
    "    inputs = self.env.sense(self)\n",
    "    deadline = self.env.get_deadline(self)\n",
    "\n",
    "    # Update state\n",
    "    self.state = (inputs['light'], inputs['oncoming'], \n",
    "                  inputs['left'], self.next_waypoint)\n",
    "\n",
    "    # update time\n",
    "    self.time += 1\n",
    "\n",
    "    # Pick the best known action\n",
    "    action = self.best_action(self.state)\n",
    "\n",
    "    # update learning rate according to \n",
    "    # number of times (state, action) pair has been seen\n",
    "    qval_pair = (self.state, action)\n",
    "    self.visits[qval_pair] = self.visits.get(qval_pair, 0) + 1\n",
    "    learn_rate = 1.0 / self.visits[qval_pair]\n",
    "\n",
    "    # Execute action and get reward\n",
    "    reward = self.env.act(self, action)\n",
    "    if reward < 0:\n",
    "        self.n_penalties += 1\n",
    "    self.reward_sum += reward\n",
    "    self.disc_reward_sum += reward / (self.time/10.0)\n",
    "\n",
    "    # Update the q-value of the (state, action) pair\n",
    "    self.qvals[qval_pair] = \\\n",
    "        (1 - learn_rate) * self.qvals.get(qval_pair, 5) + \\\n",
    "        learn_rate * reward "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's see the results for the optimistic agent with this new learning rate decay:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reward_sum</th>\n",
       "      <th>disc_reward_sum</th>\n",
       "      <th>n_dest_reached</th>\n",
       "      <th>last_dest_fail</th>\n",
       "      <th>sum_time_left</th>\n",
       "      <th>n_penalties</th>\n",
       "      <th>last_penalty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2217.265000</td>\n",
       "      <td>70.084863</td>\n",
       "      <td>98.930000</td>\n",
       "      <td>19.860000</td>\n",
       "      <td>1731.900000</td>\n",
       "      <td>37.040000</td>\n",
       "      <td>93.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>44.469514</td>\n",
       "      <td>14.477370</td>\n",
       "      <td>1.017573</td>\n",
       "      <td>31.017434</td>\n",
       "      <td>80.620886</td>\n",
       "      <td>5.572153</td>\n",
       "      <td>6.017953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2126.000000</td>\n",
       "      <td>44.277556</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1541.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>69.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2182.250000</td>\n",
       "      <td>58.903426</td>\n",
       "      <td>98.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1678.500000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>91.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2211.500000</td>\n",
       "      <td>69.294894</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1725.000000</td>\n",
       "      <td>36.500000</td>\n",
       "      <td>95.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2244.750000</td>\n",
       "      <td>78.495404</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>34.750000</td>\n",
       "      <td>1784.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>99.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2338.000000</td>\n",
       "      <td>130.943204</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>1941.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        reward_sum  disc_reward_sum  n_dest_reached  last_dest_fail  \\\n",
       "count   100.000000       100.000000      100.000000      100.000000   \n",
       "mean   2217.265000        70.084863       98.930000       19.860000   \n",
       "std      44.469514        14.477370        1.017573       31.017434   \n",
       "min    2126.000000        44.277556       95.000000        0.000000   \n",
       "25%    2182.250000        58.903426       98.750000        0.000000   \n",
       "50%    2211.500000        69.294894       99.000000        1.000000   \n",
       "75%    2244.750000        78.495404      100.000000       34.750000   \n",
       "max    2338.000000       130.943204      100.000000       98.000000   \n",
       "\n",
       "       sum_time_left  n_penalties  last_penalty  \n",
       "count     100.000000   100.000000    100.000000  \n",
       "mean     1731.900000    37.040000     93.920000  \n",
       "std        80.620886     5.572153      6.017953  \n",
       "min      1541.000000    25.000000     69.000000  \n",
       "25%      1678.500000    34.000000     91.000000  \n",
       "50%      1725.000000    36.500000     95.000000  \n",
       "75%      1784.000000    39.000000     99.000000  \n",
       "max      1941.000000    75.000000    100.000000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_optim_new_learn_rate = pd.DataFrame.from_csv(\"optim_new_learn_rate_agent_results.csv\")\n",
    "df_optim_new_learn_rate.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The last task, according to the project description, is:\n",
    "\n",
    "> Apply the reinforcement learning techniques you have learnt, and tweak the parameters (e.g. learning rate, discount factor, action selection method, etc.), to improve the performance of your agent. Your goal is to get it to a point so that within 100 trials, the agent is able to learn a feasible policy - i.e. reach the destination within the allotted time, with net reward remaining positive.\n",
    "\n",
    "This goal is reached with the implementation presented above. Although there are instances in which, with 90+ trials, the agent will pick a wrong action from time to time, in my tests it never failed to reach the destination after 60+ trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0xb9d30b8>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAECCAYAAAASDQdFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xd5049b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "#plt.rcParams['figure.figsize'] = [15, 10]\n",
    "\n",
    "plt.plot(df_original['last_penalty'])\n",
    "\n",
    "#for column in df_original.columns:\n",
    "#    sns.boxplot(x=column, data=df_original)\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
